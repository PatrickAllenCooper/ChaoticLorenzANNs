{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTpc5O-E2Umd"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBkqXrKDSKUG"
      },
      "outputs": [],
      "source": [
        "! pip install pytorch-lightning wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9LFiQuF7I0B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import wandb\n",
        "import subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJHSbfhYlEz6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5Lw0swjXjZp"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeGTqVijXlun"
      },
      "outputs": [],
      "source": [
        "MLP = True\n",
        "ATN = False\n",
        "RNN = False\n",
        "\n",
        "nine_six = False\n",
        "\n",
        "tsteps = 3\n",
        "\n",
        "MAX_EPOCHS = 3000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zOXEPRG2Xvg"
      },
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORDwafEqRAqn"
      },
      "outputs": [],
      "source": [
        "\n",
        "if MLP:\n",
        "  # Define the MLP model\n",
        "  class Generator(nn.Module):\n",
        "      def __init__(self, input_size, hidden_size, output_size):\n",
        "          super(Generator, self).__init__()\n",
        "          self.layers = nn.Sequential(\n",
        "              nn.Linear(input_size, hidden_size),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(hidden_size, hidden_size),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(hidden_size, output_size)\n",
        "          )\n",
        "\n",
        "      def forward(self, x):\n",
        "          return self.layers(x)\n",
        "\n",
        "if ATN:\n",
        "  class Generator(nn.Module):\n",
        "      def __init__(self, input_size, hidden_size, output_size, nhead=16, num_layers=3):\n",
        "          super(Generator, self).__init__()\n",
        "\n",
        "          self.embedding = nn.Linear(input_size, hidden_size)\n",
        "          self.transformer = nn.Transformer(hidden_size, nhead, num_layers)\n",
        "          self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "      def forward(self, src, tgt):\n",
        "        src = self.embedding(src)\n",
        "        tgt = self.embedding(tgt)\n",
        "        x = self.transformer(src, tgt)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "if RNN:\n",
        "  class LSTMGenerator(nn.Module):\n",
        "      def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "          super(LSTMGenerator, self).__init__()\n",
        "          self.hidden_size = hidden_size\n",
        "          self.num_layers = num_layers\n",
        "\n",
        "          self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "          self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "      def forward(self, x, hidden=None):\n",
        "          if hidden is None:\n",
        "              hidden = self.init_hidden(x.size(0))\n",
        "\n",
        "          out, hidden = self.lstm(x, hidden)\n",
        "          out = self.fc(out)\n",
        "\n",
        "          return out, hidden\n",
        "\n",
        "      def init_hidden(self, batch_size):\n",
        "          h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "          c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "          return (h0, c0)\n",
        "\n",
        "if MLP or RNN:\n",
        "  class LorenzSystemModule(pl.LightningModule):\n",
        "      def __init__(self, model, learning_rate=1e-3):\n",
        "          super().__init__()\n",
        "          self.model = model\n",
        "          self.learning_rate = learning_rate\n",
        "          self.loss_function = nn.MSELoss()\n",
        "\n",
        "      def forward(self, x):\n",
        "          return self.model(x)\n",
        "\n",
        "      def training_step(self, batch, batch_idx):\n",
        "          x, y = batch\n",
        "          y_pred = self(x)\n",
        "          loss = self.loss_function(y_pred, y)\n",
        "          self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "          wandb.log({\"train_loss\": loss})\n",
        "          return loss\n",
        "\n",
        "      def validation_step(self, batch, batch_idx):\n",
        "          x, y = batch\n",
        "          y_pred = self(x)\n",
        "          loss = self.loss_function(y_pred, y)\n",
        "          self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "          wandb.log({\"val_loss\": loss})\n",
        "          return loss\n",
        "\n",
        "      def configure_optimizers(self):\n",
        "          optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "          return optimizer\n",
        "\n",
        "if ATN:\n",
        "  class LorenzSystemModule(pl.LightningModule):\n",
        "      def __init__(self, model, learning_rate=1e-3):\n",
        "          super().__init__()\n",
        "          self.model = model\n",
        "          self.learning_rate = learning_rate\n",
        "          self.loss_function = nn.MSELoss()\n",
        "\n",
        "      def forward(self, x, y):\n",
        "          return self.model(x, y)\n",
        "\n",
        "      def training_step(self, batch, batch_idx):\n",
        "          x, y = batch\n",
        "          y_pred = self(x, y)\n",
        "          print(\"REAL\")\n",
        "          print(y)\n",
        "          print(\"FAKE\")\n",
        "          print(y_pred)\n",
        "          loss = self.loss_function(y_pred, y)\n",
        "          self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "          wandb.log({\"train_loss\": loss})\n",
        "          return loss\n",
        "\n",
        "      def validation_step(self, batch, batch_idx):\n",
        "          x, y = batch\n",
        "          y_pred = self(x, y)\n",
        "          loss = self.loss_function(y_pred, y)\n",
        "          self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "          wandb.log({\"val_loss\": loss})\n",
        "          return loss\n",
        "\n",
        "      def configure_optimizers(self):\n",
        "          optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "          return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLcM3A0K2ZN0"
      },
      "source": [
        "## Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQSM-JVmopUR"
      },
      "outputs": [],
      "source": [
        "from scipy.integrate import odeint\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def execute_experiment(experiment_config):\n",
        "  experiment_title = experiment_config[0]\n",
        "  Sparsity = experiment_config[1]\n",
        "  wandb.init(name=experiment_title, project=\"chaotic-rnn\", entity=\"patrickallencooper\")\n",
        "\n",
        "  hidden = 64\n",
        "\n",
        "  USE_SPARSE = True\n",
        "\n",
        "  if nine_six:\n",
        "    def lorenz_system(T, dt, N=3, F=20, init_condition=None):\n",
        "      \n",
        "      def lorenz_96_deriv(X, t0, N=N, F=F):\n",
        "          dXdt = np.zeros(N)\n",
        "          for i in range(N):\n",
        "              dXdt[i] = (X[(i+1) % N] - X[(i-2) % N]) * X[(i-1) % N] - X[i] + F\n",
        "          return dXdt\n",
        "\n",
        "      if init_condition is None:\n",
        "          init_condition = [1.0, 1.0, 1.0]\n",
        "\n",
        "      time_points = np.arange(0, T, dt)\n",
        "      trajectory = odeint(lorenz_96_deriv, init_condition, time_points)\n",
        "      return trajectory.T\n",
        "\n",
        "  else:\n",
        "    def lorenz_system(T, dt, init_condition=None):\n",
        "        sigma, beta, rho = 10, 2.667, 28\n",
        "\n",
        "        def lorenz_deriv(X, t0, sigma=sigma, beta=beta, rho=rho):\n",
        "            x, y, z = X\n",
        "            return [sigma * (y - x), x * (rho - z) - y, x * y - beta * z]\n",
        "\n",
        "        if init_condition is None:\n",
        "            init_condition = [1.0, 1.0, 1.0]\n",
        "\n",
        "        time_points = np.arange(0, T, dt)\n",
        "        trajectory = odeint(lorenz_deriv, init_condition, time_points)\n",
        "        x, y, z = trajectory.T\n",
        "        return x, y, z\n",
        "\n",
        "  def generate_initial_conditions(n_points, x_range, y_range, z_range):\n",
        "      x0 = np.random.uniform(x_range[0], x_range[1], n_points)\n",
        "      y0 = np.random.uniform(y_range[0], y_range[1], n_points)\n",
        "      z0 = np.random.uniform(z_range[0], z_range[1], n_points)\n",
        "      return np.array([x0, y0, z0]).T\n",
        "\n",
        "  # Generate the Lorenz system data\n",
        "  n_points = 1\n",
        "  x_range = [-20.0, 20.0]\n",
        "  y_range = [-20.0, 20.0]\n",
        "  z_range = [-20.0, 20.0]\n",
        "  T, dt = 100, 0.005\n",
        "\n",
        "  print(\"Generating Initial Conditions:\")\n",
        "  initial_conditions = generate_initial_conditions(n_points, x_range, y_range, z_range)\n",
        "\n",
        "  print(\"Integrating trajectories:\")\n",
        "  data_list = []\n",
        "  for init_condition in initial_conditions:\n",
        "      x, y, z = lorenz_system(T, dt, init_condition)\n",
        "      data = np.array([x[:-1], y[:-1], z[:-1], x[1:], y[1:], z[1:]]).T\n",
        "      data_list.append(data)\n",
        "\n",
        "  data = np.concatenate(data_list, axis=0)\n",
        "\n",
        "  scaler = MinMaxScaler()\n",
        "  data = scaler.fit_transform(data)\n",
        "\n",
        "  print(\"Pre Sparsity Length: \" + str(len(data)))\n",
        "  if USE_SPARSE:\n",
        "    data = data[::Sparsity]\n",
        "    print(\"Post Sparsity Length: \" + str(len(data)))\n",
        "  train_data_pre = torch.Tensor(data)\n",
        "\n",
        "  train_data, test_data = train_data_pre[:int(len(train_data_pre) * 0.8)], train_data_pre[int(len(train_data_pre) * 0.8):]\n",
        "  train_data_sparse = train_data\n",
        "  test_data_sparse = test_data\n",
        "\n",
        "  # Split the data into training and test datasets\n",
        "  train_dataset_sparse = TensorDataset(torch.Tensor(train_data_sparse[:, :tsteps]), torch.Tensor(train_data_sparse[:, tsteps:]))\n",
        "  test_dataset_sparse = TensorDataset(torch.Tensor(test_data_sparse[:, :tsteps]), torch.Tensor(test_data_sparse[:, tsteps:]))\n",
        "\n",
        "  train_loader = DataLoader(train_dataset_sparse, batch_size=2048, shuffle=True)\n",
        "  val_loader = DataLoader(test_dataset_sparse, batch_size=2048, shuffle=False)\n",
        "\n",
        "  input_size, hidden_size, output_size = tsteps, hidden, tsteps\n",
        "  model = Generator(input_size, hidden_size, output_size)\n",
        "  lorenz_system_module = LorenzSystemModule(model)\n",
        "\n",
        "  # Train the model and compute validation loss\n",
        "  trainer = pl.Trainer(max_epochs=MAX_EPOCHS)\n",
        "  trainer.fit(lorenz_system_module, train_loader, val_loader)\n",
        "\n",
        "  model_scripted = torch.jit.script(model)\n",
        "  model_scripted.save(experiment_title + '.pt')\n",
        "\n",
        "  model_cloud = '! cp ' + experiment_title + '.pt drive/MyDrive/\"Colab Notebooks\"/Classes/\"Chaotic Dynamics\"/\"Models\"'\n",
        "\n",
        "  subprocess.run(model_cloud, stdout=subprocess.PIPE, shell=True)\n",
        "\n",
        "  # Test the model\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      initial_state = torch.Tensor(train_data[0, :tsteps]).unsqueeze(0)\n",
        "      predicted_trajectory = [initial_state]\n",
        "      for _ in range(len(train_data) - 1):\n",
        "        if MLP:\n",
        "          prediction = model(predicted_trajectory[-1])\n",
        "        else:\n",
        "          prediction = model(predicted_trajectory[-1], predicted_trajectory[0])\n",
        "        predicted_trajectory.append(prediction)\n",
        "\n",
        "  predicted_trajectory = torch.cat(predicted_trajectory).detach().numpy()\n",
        "  #predicted_trajectory = scaler.inverse_transform(predicted_trajectory)\n",
        "\n",
        "  # Plot the predicted and true Lorenz system trajectories\n",
        "  fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "  ax1 = fig.add_subplot(121, projection='3d')\n",
        "  ax1.plot(x, y, z, linewidth=1, color='red')\n",
        "  ax1.set_title(\"True Lorenz System Trajectory\")\n",
        "  ax1.set_xlabel(\"X\")\n",
        "  ax1.set_ylabel(\"Y\")\n",
        "  ax1.set_zlabel(\"Z\")\n",
        "\n",
        "  ax2 = fig.add_subplot(122, projection='3d')\n",
        "  ax2.plot(predicted_trajectory[:, 0], predicted_trajectory[:, 1], predicted_trajectory[:, 2], linewidth=1, color='blue')\n",
        "  ax2.set_title(\"Predicted Lorenz System Trajectory\")\n",
        "  ax2.set_xlabel(\"X\")\n",
        "  ax2.set_ylabel(\"Y\")\n",
        "  ax2.set_zlabel(\"Z\")\n",
        "\n",
        "  plt.savefig(experiment_title + \".png\", dpi=300)\n",
        "\n",
        "  image_cloud = '! cp ' + experiment_title + '.png drive/MyDrive/\"Colab Notebooks\"/Classes/\"Chaotic Dynamics\"/\"Images\"'\n",
        "\n",
        "  subprocess.run(image_cloud, stdout=subprocess.PIPE, shell=True)\n",
        "\n",
        "  # Test the model with extended timesteps\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      initial_state = torch.Tensor(test_data[0, :tsteps]).unsqueeze(0)\n",
        "      predicted_trajectory_test = [initial_state]\n",
        "      for _ in range(len(test_data) - 1):\n",
        "        if MLP:\n",
        "          prediction = model(predicted_trajectory_test[-1])\n",
        "        else:\n",
        "          prediction = model(predicted_trajectory_test[-1], predicted_trajectory_test[0])\n",
        "        predicted_trajectory_test.append(prediction)\n",
        "\n",
        "  predicted_trajectory_test = torch.cat(predicted_trajectory_test).detach().numpy()\n",
        "  #predicted_trajectory_test = scaler.inverse_transform(predicted_trajectory_test)\n",
        "\n",
        "  # Plot the predicted and true Lorenz system trajectories with extended timesteps\n",
        "  fig = plt.figure(figsize=(18, 6))\n",
        "\n",
        "  ax1 = fig.add_subplot(131, projection='3d')\n",
        "  ax1.plot(test_data[:, 0], test_data[:, 1], test_data[:, 2], linewidth=1, color='red')\n",
        "  ax1.set_title(\"True Lorenz System Trajectory (Extended)\")\n",
        "  ax1.set_xlabel(\"X\")\n",
        "  ax1.set_ylabel(\"Y\")\n",
        "  ax1.set_zlabel(\"Z\")\n",
        "\n",
        "  ax2 = fig.add_subplot(132, projection='3d')\n",
        "  ax2.plot(predicted_trajectory[:, 0], predicted_trajectory[:, 1], predicted_trajectory[:, 2], linewidth=1, color='blue')\n",
        "  ax2.set_title(\"Predicted Lorenz System Trajectory\")\n",
        "  ax2.set_xlabel(\"X\")\n",
        "  ax2.set_ylabel(\"Y\")\n",
        "  ax2.set_zlabel(\"Z\")\n",
        "\n",
        "  ax3 = fig.add_subplot(133, projection='3d')\n",
        "  ax3.plot(predicted_trajectory_test[:, 0], predicted_trajectory_test[:, 1], predicted_trajectory_test[:, 2], linewidth=1, color='green')\n",
        "  ax3.set_title(\"Predicted Lorenz System Trajectory (Extended)\")\n",
        "  ax3.set_xlabel(\"X\")\n",
        "  ax3.set_ylabel(\"Y\")\n",
        "  ax3.set_zlabel(\"Z\")\n",
        "\n",
        "  plt.savefig(experiment_title + \"_extended.png\", dpi=300)\n",
        "\n",
        "  image_exd_cloud = '! cp ' + experiment_title + '_extended.png drive/MyDrive/\"Colab Notebooks\"/Classes/\"Chaotic Dynamics\"/\"Images\"'\n",
        "\n",
        "  subprocess.run(image_exd_cloud, stdout=subprocess.PIPE, shell=True)\n",
        "\n",
        "  # calculate the mse for the rk method and push to log file\n",
        "  #rk_mse = torch.nn.functional.mse_loss(predicted_trajectory_test, test_data)\n",
        "  #rk_mse_short = torch.nn.functional.mse_loss(predicted_trajectory_test[:1000], test_data[:1000])\n",
        "  #print(rk_mse)\n",
        "  #print(rk_mse_short)\n",
        "\n",
        "  # Test the model against MTU\n",
        "  truth_comparison = train_data[100:1100, :tsteps].numpy()\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      initial_state = torch.Tensor(train_data[100, :tsteps]).unsqueeze(0)\n",
        "      predicted_trajectory_test_mse = [initial_state]\n",
        "      for _ in range(1000 - 1):\n",
        "        if MLP:\n",
        "          prediction = model(predicted_trajectory_test_mse[-1])\n",
        "        else:\n",
        "          prediction = model(predicted_trajectory_test_mse[-1], predicted_trajectory_test_mse[0])\n",
        "        predicted_trajectory_test_mse.append(prediction)\n",
        "\n",
        "  predicted_trajectory_test_mse = torch.cat(predicted_trajectory_test_mse).detach().numpy()\n",
        "  #predicted_trajectory_test_mse = scaler.inverse_transform(predicted_trajectory_test_mse)\n",
        "  # Calculate the mean squared error (MSE) for each entry\n",
        "  print(predicted_trajectory_test_mse.shape)\n",
        "  print(truth_comparison.shape)\n",
        "  mse = (predicted_trajectory_test_mse - truth_comparison) ** 2\n",
        "\n",
        "  #mse = np.cumsum(mse)\n",
        "\n",
        "  # Find the indices where the error grows above 0.3\n",
        "  error_above_03_indices = np.nonzero(mse > 0.3)\n",
        "\n",
        "  plt.figure()\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # Plot the data\n",
        "  ax.plot(range(len(mse)), mse, label=['X','Y','Z'])\n",
        "\n",
        "  # Set custom X tick positions and labels\n",
        "  custom_xticks_positions = [200, 400, 600, 800, 1000]\n",
        "  custom_xticks_labels = ['1', '2', '3', '4', '5']\n",
        "  ax.set_xticks(custom_xticks_positions)\n",
        "  ax.set_xticklabels(custom_xticks_labels)\n",
        "\n",
        "  # Plot the MSE values\n",
        "  ax.set_xlabel('MTU')\n",
        "  ax.set_ylabel('MSE')\n",
        "\n",
        "  ax.axhline(y=0.3, linestyle='--', color='r')\n",
        "\n",
        "  # Set the legend and display the plot\n",
        "  plt.legend()\n",
        "  \n",
        "  plt.savefig(experiment_title + \"_mse.png\", dpi=300)\n",
        "\n",
        "  image_mse_cloud = '! cp ' + experiment_title + '_mse.png drive/MyDrive/\"Colab Notebooks\"/Classes/\"Chaotic Dynamics\"/\"Images\"'\n",
        "\n",
        "  subprocess.run(image_mse_cloud, stdout=subprocess.PIPE, shell=True)\n",
        "\n",
        "  # DO SO FOR SUMMED ARRAY\n",
        "  mse = (np.sum(predicted_trajectory_test_mse, axis=1) - np.sum(truth_comparison, axis=1)) ** 2\n",
        "\n",
        "  # Find the indices where the error grows above 0.3\n",
        "  error_above_03_indices = np.nonzero(mse > 0.3)\n",
        "\n",
        "  plt.figure()\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # Plot the data\n",
        "  ax.plot(range(len(mse)), mse, label=['Trajectory MSE'])\n",
        "\n",
        "  # Set custom X tick positions and labels\n",
        "  custom_xticks_positions = [200, 400, 600, 800, 1000]\n",
        "  custom_xticks_labels = ['1', '2', '3', '4', '5']\n",
        "  ax.set_xticks(custom_xticks_positions)\n",
        "  ax.set_xticklabels(custom_xticks_labels)\n",
        "\n",
        "  # Plot the MSE values\n",
        "  ax.set_xlabel('MTU')\n",
        "  ax.set_ylabel('MSE')\n",
        "\n",
        "  ax.axhline(y=0.3, linestyle='--', color='r')\n",
        "\n",
        "  # Highlight the points where the error grows above 0.3\n",
        "  #plt.scatter(error_above_03_indices, mse[error_above_03_indices], color='red', label='Error > 0.3')\n",
        "\n",
        "  # Set the legend and display the plot\n",
        "  plt.legend()\n",
        "  \n",
        "  plt.savefig(experiment_title + \"_mse_sum.png\", dpi=300)\n",
        "\n",
        "  image_mse_cloud = '! cp ' + experiment_title + '_mse_sum.png drive/MyDrive/\"Colab Notebooks\"/Classes/\"Chaotic Dynamics\"/\"Images\"'\n",
        "\n",
        "  subprocess.run(image_mse_cloud, stdout=subprocess.PIPE, shell=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS1Vf3dm2av5"
      },
      "source": [
        "## Control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vbhpJdTr0htT"
      },
      "outputs": [],
      "source": [
        "if MLP:\n",
        "  experiment_configuration = [(\"final_CMLP_epoch_\" + str(MAX_EPOCHS) + \"_hidden_64_lr_1e-3_dt_0.01_sparse_1\", 1),\n",
        "                              (\"final_CMLP_epoch_\" + str(MAX_EPOCHS) + \"_hidden_64_lr_1e-3_dt_0.01_sparse_10\", 2),\n",
        "                              (\"final_CMLP_epoch_\" + str(MAX_EPOCHS) + \"_hidden_64_lr_1e-3_dt_0.01_sparse_20\", 5),\n",
        "                              (\"final_CMLP_epoch_\" + str(MAX_EPOCHS) + \"_hidden_64_lr_1e-3_dt_0.01_sparse_40\", 10),\n",
        "                              (\"final_CMLP_epoch_\" + str(MAX_EPOCHS) + \"_hidden_64_lr_1e-3_dt_0.01_sparse_60\", 15),\n",
        "                              (\"final_CMLP_epoch_\" + str(MAX_EPOCHS) + \"_hidden_64_lr_1e-3_dt_0.01_sparse_80\", 20)]\n",
        "\n",
        "if ATN:\n",
        "  experiment_configuration = [(\"ATN_epoch_\" + str(MAX_EPOCHS) + \"_hidden_64_lr_1e-3_dt_0.01_sparse_1\", 1),\n",
        "                              (\"ATN_epoch_\" + str(MAX_EPOCHS) + \"_hidden_64_lr_1e-3_dt_0.01_sparse_10\", 2),\n",
        "                              (\"ATN_epoch_\" + str(MAX_EPOCHS) + \"_hidden_64_lr_1e-3_dt_0.01_sparse_20\", 5),\n",
        "                              (\"ATN_epoch_\" + str(MAX_EPOCHS) + \"_hidden_64_lr_1e-3_dt_0.01_sparse_40\", 10),\n",
        "                              (\"ATN_epoch_\" + str(MAX_EPOCHS) + \"_hidden_64_lr_1e-3_dt_0.01_sparse_60\", 15),\n",
        "                              (\"ATN_epoch_\" + str(MAX_EPOCHS) + \"_hidden_64_lr_1e-3_dt_0.01_sparse_80\", 20)]\n",
        "\n",
        "if RNN:\n",
        "  experiment_configuration = [(\"RNN_epoch_\" + str(MAX_EPOCHS) + \"_hidden_64_lr_1e-3_dt_0.01_sparse_1\", 1),\n",
        "                              (\"RNN_epoch_\" + str(MAX_EPOCHS) + \"_hidden_64_lr_1e-3_dt_0.01_sparse_10\", 2),\n",
        "                              (\"RNN_epoch_\" + str(MAX_EPOCHS) + \"_hidden_64_lr_1e-3_dt_0.01_sparse_20\", 5),\n",
        "                              (\"RNN_epoch_\" + str(MAX_EPOCHS) + \"_hidden_64_lr_1e-3_dt_0.01_sparse_40\", 10),\n",
        "                              (\"RNN_epoch_\" + str(MAX_EPOCHS) + \"_hidden_64_lr_1e-3_dt_0.01_sparse_60\", 15),\n",
        "                              (\"RNN_epoch_\" + str(MAX_EPOCHS) + \"_hidden_64_lr_1e-3_dt_0.01_sparse_80\", 20)]\n",
        "\n",
        "for experiment_config in experiment_configuration:\n",
        "  execute_experiment(experiment_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzhPuL_LlKLR"
      },
      "source": [
        "The research question here, is how well does the neural network generalize to unseen points within a chaotic system, in this particular case Lorenz, given a very sparse sampling.\n",
        "\n",
        "This is not strictly speaking a 'fair' comparison as one has access to historical data about the system, but you can imagine a scenario where you have specific guarantees about a system up until a point and you would like to extrapolate beyond, can a neural network go further while preserving the dynamics of a system. In terms of utility this may be a bit contrived, but I think it does provide some sense the ability to model a chaotic system as a timeseries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0WiC3wqlshw"
      },
      "source": [
        "Step 1: Train a model on very very few points using dense RK as baseline.\n",
        "\n",
        "Step 2: Run RK on a Comparable number of points.\n",
        "\n",
        "Step 3: Compare results.\n",
        "\n",
        "Chart outcomes, explain under what conditions such a model could be useful."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}